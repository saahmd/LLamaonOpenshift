---
- name: Send API request and debug complete response
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Create agent
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: |
          {
            "agent_config": {
              "sampling_params": {
                "strategy": {"type": "greedy"},
                "max_tokens": 512
              },
              "toolgroups": ["mcp::aap"],
              "tool_config": {"tool_choice": "auto"},
              "model": "llama-3-2-3b",
              "instructions": "You are a helpful assistant. When you use a tool always respond with a summary of the result.",
              "enable_session_persistence": false
            }
          }
        body_format: json
        return_content: yes
        validate_certs: no
      register: agent_response

      
    - name: Display Agent response
      ansible.builtin.debug:
        msg: "{{ agent_response.json }}"
        
    - name: Create session
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response.json.agent_id  }}/session"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: '{"session_name": "string"}'
        body_format: json
        return_content: yes
        validate_certs: no
      register: session_response

    - name: Display Session ID
      ansible.builtin.debug:
        msg: "{{ session_response.json }}"

    - name: Copy Python streaming script
      copy:
        src: "../files/llm_turn_stream.py"
        dest: "/tmp/llm_turn_stream.py"
        mode: '0755'

    - name: Run streaming turn via Python
      command: >
        python3 /tmp/llm_turn_stream.py
        {{ agent_response.json.agent_id }}
        {{ session_response.json.session_id }}
        "What is the weather in Hyd"
      register: turn_output

    - name: Display merged LLM response from file
      slurp:
        src: /tmp/llm_turn_response.txt
      register: llm_file

    - name: Show full LLM output
      debug:
        msg: "{{ llm_file.content | b64decode }}"
