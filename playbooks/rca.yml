---
- name: Agent → Session → LLM Turn (Python streaming)
  hosts: localhost
  gather_facts: no

  tasks:
    - name: Create agent for RCA
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: |
         {
            "agent_config": {
             "sampling_params": {"strategy": {"type": "greedy"}, "max_tokens": 3000},
              "model": "granite-8b-lab-v1",
              "instructions": "You are a helpful assistant that analyzes logs.Based on the logs below, Provide a single, concise fix instruction (1-2 lines).",
              "enable_session_persistence": false
            }
          }
        body_format: json
        return_content: yes
        validate_certs: no
      register: agent_response

    - name: Create session
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response.json.agent_id }}/session"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: '{"session_name": "string"}'
        body_format: json
        return_content: yes
        validate_certs: no
      register: session_response

    - name: Create turn
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response.json.agent_id }}/session/{{ session_response.json.session_id }}/turn"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: |
          {
            "messages": [
              {
                "role": "user",
                "content": "{{ gpt_prompt | default('What is the capital of the USA?') }}",
                "context": "string"
              }
            ],
            "stream": true,
            "tool_config": {
              "tool_choice": "auto"
            }
          }
        body_format: json
        return_content: yes
        validate_certs: no
      register: turn_response_gpt

    - name: Display Turn response
      ansible.builtin.debug:
        msg: "{{ turn_response_gpt.content }}"

    - name: Extract GPT Response as string
      ansible.builtin.set_fact:
        gpt_response: "{{ (turn_response_gpt.content | regex_search('\"output_message\".*?\"content\":\"([^\"]+)\"', '\\1'))[0] }}"

    - name: Display GPT Response
      ansible.builtin.debug:
        msg: "{{ gpt_response }}"


    - name: Create turn for RCA
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response.json.agent_id }}/session/{{ session_response.json.session_id }}/turn"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body_format: json
        body:
          messages:
            - role: "user"
              content: "{{ rca_prompt | default('What is the capital of the USA?') }}"
              context: "string"
          stream: true
          tool_config:
            tool_choice: "auto"
        return_content: yes
        validate_certs: no
      register: turn_response_rca

    - name: Display Turn response
      ansible.builtin.debug:
        msg: "{{ turn_response_rca.content }}"

    - name: Extract RCA Response
      ansible.builtin.set_fact:
        rca_response: "{{ turn_response_rca.content | regex_search('\"output_message\".*?\"content\":\"([^\"]+)\"', '\\1') }}"

    - name: Display RCA Response
      ansible.builtin.debug:
        msg: "{{ rca_response }}"

    
    - name: Create agent for Decision
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body_format: json
        body:
          agent_config:
            sampling_params:
              strategy:
                type: "greedy"
              max_tokens: 512
            toolgroups:
              - "mcp::aap"
            tool_config:
              tool_choice: "auto"
            model: "llama-3-2-3b"
            instructions: >
              You are a helpful assistant responsible for identifying the most relevant Ansible job template based on incoming log data.

              You have access to the AAP tool, and you must strictly use the function list_job_templates() to retrieve the list of all available job templates.

              Follow these steps:
              1. Read and analyze the logs provided in the user query.
              2. Use list_job_templates() to get the available templates.
              3. Match the most relevant job template name to the log content.
                - Matching should be case-insensitive.
                - Prefer exact matches; if none are found, allow close matches.
              4. ONLY respond "No playbook found. User intervention required." if the search truly returns no relevant results.
              
              Bias toward finding matches - if there's any reasonable playbook, use it.
              Do not provide explanations or additional text in the output.
              Use the AAP tool only to get the template list — do not guess template names.
        return_content: yes
        validate_certs: no
      register: agent_response_decision


    - name: Create session
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response_decision.json.agent_id }}/session"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body: '{"session_name": "string"}'
        body_format: json
        return_content: yes
        validate_certs: no
      register: session_response_decision


    - name: Create turn
      uri:
        url: "https://llamastack-server-llama-serve.apps.cluster-bgzcw.bgzcw.sandbox942.opentlc.com/v1/agents/{{ agent_response_decision.json.agent_id }}/session/{{ session_response_decision.json.session_id }}/turn"
        method: POST
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
        body_format: json
        body:
          messages:
            - role: "user"
              #content: "{{ 'Remove Invalid Directive. Tell me which playbook can resolve this issue' }}"
              content: " The Apache HTTP Server on node1 is failing to start due to an invalid directive in the configuration file. Specifically, the directive 'InvalidDirectiveHere' is causing the main process to exit with a non-zero status code. This error message indicates that the directive is either misspelled or not defined in the server configuration. To resolve this issue, the invalid directive should be located and corrected or removed from the Apache configuration file. After making the necessary changes, the httpd service should be restarted to apply the updates and ensure that the Apache server is running as expected "
          context: "string"
          stream: true
          tool_config:
            tool_choice: "auto"
        return_content: yes
        validate_certs: no
      register: turn_response_decision


    - name: Display Turn response
      ansible.builtin.debug:
        msg: "{{ turn_response_decision.content }}"

    - name: Extract Decision Response
      ansible.builtin.set_fact:
        decision_response: "{{ turn_response_decision.content | regex_search('\"output_message\".*?\"content\":\"((?:\\\\.|[^\"])*)\"', '\\1') }}"
        
    - name: Display RCA Response
      ansible.builtin.debug:
        msg: "{{ decision_response }}"


    - name: Set stats for next controller job
      ansible.builtin.set_stats:
        data:
          gpt_response: "{{ gpt_response }}"
          rca_response: "{{ rca_response }}"